{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e90bc06",
   "metadata": {},
   "source": [
    "## Getting Started \n",
    "\n",
    "Download the PPP LOANS file and the NAICS.csv file in order to do this tutorial."
    "\n",
    "Before we start working with data, we need to tell the computer what python libraries we are going to use. Think of libraries as toolkits. Some examples (to name just a few!):\n",
    "\n",
    "<b>Pandas:</b> filtering, sorting, aggregating data (think pivot tables but in here they’re called ‘group bys’), math, merging datasets, appending datasets and more! \n",
    "\n",
    "<b>Geopandas:</b> analyze spatial files that feed into maps. Merging together two datasets based on the location of your records. Creates spatial files.\n",
    "\n",
    "<b>Numpy:</b> good for more complex math. We’ll use this for if/then statements\n",
    "\n",
    "<b>Beautiful Soup:</b> good for scraping websites. \n",
    "\n",
    "Some libraries come automatically come with the Python that you have downloaded on your computers. Others we will need to download through the command line, but we will deal with that when we get there. \n",
    "\n",
    "In order to invoke a library you plan to use for your jupyter notebook file, you simply write \"import\" followed by the name of the library.\n",
    "\n",
    "We will be using Pandas, so let's start there. In the cell below, type:\n",
    "\n",
    "<i>import pandas as pd</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c178d8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "854be8c5",
   "metadata": {},
   "source": [
    "You should have seen a little asterisk pop up and disapeer to the left of the above cell, and then nothing else. That means it works! (The asterisk appears while the code is running. For really complex lines of code you will see that asterisk for much longer period of time. Think of it as the spinning beach ball when your Mac is loading something.) \n",
    "\n",
    "So what we just did here was:\n",
    "\n",
    "A. Tell the computer to import pandas so it knows what tools you might use \n",
    "\n",
    "B. Abbreviated the name of pandas so when you write it later you don't have to spell out pandas every time. The computer will recognize \"pd\" as the pandas library from here on out. You can choose the name here. Like, you could call it bananas and the computer would recognize bananas as the pandas library.\n",
    "\n",
    "Now that we've set the table, let's bring in the dataset we plan to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b125b3e",
   "metadata": {},
   "source": [
    "## Loading your datasets \n",
    "\n",
    "Just as you would open a spreadsheet to start analyzing it in Excel, you need to open the files you plan to analyze in your jupyter notebooks. You do this not by clicking on a file name. Instead, we tell the computer to read the file.  \n",
    "\n",
    "Make sure you’ve opened your jupyter notebook in the same folder where your data lives, or the computer won’t be able to find it!\n",
    "\n",
    "Try writing the following code in the cell below.\n",
    "\n",
    "<i>pd.read_csv('public_150k_plus.csv')</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38501da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99a5b884",
   "metadata": {},
   "source": [
    "The computer should have spit out a view of your data. (The first five rows and the last five rows.) It’s nice to see what our data looks like, but if we want to continue working with it, we need to assign it a name. In other words, we need to make it a variable.\n",
    "\n",
    "## Variables\n",
    "\n",
    "A Python variable is a reserved memory location to store values. Think of variables as names you assign to represent a dataframe, a column within that dataframe, a list of numbers or words or even a math formula.\n",
    "\n",
    "<ul>\n",
    "<li>Variables must start with a letter</li> \n",
    "<li>No special characters</li> \n",
    "<li>Case sensitive</li> \n",
    "</ul>\n",
    "\n",
    "Once you assign a variable, you can use that name in future commands and the computer will understand that it represents the values you assigned to it. You do this by assigning a name, typing a single = and then following with whatever it represents. \n",
    "\n",
    "For example, let's name our PPP loan data 'df' (short for dataframe - but you could literally name it anything you want): \n",
    "\n",
    "<i>df = pd.read_csv('public_150k_plus.csv')</i>\n",
    "\n",
    "Note that the value of a variable, if it is text, should be in quotes or single quotes. If the value is a number, you don’t need to put it in quotes. But we’ll get to that later. \n",
    "\n",
    "Try that code in the cell below. Notice how it looks like nothing happens? That usually means the command worked! The computer only does what it is told, so you have to tell it that you want to see your results if that’s what you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011a91e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d58eaa2",
   "metadata": {},
   "source": [
    "In order to do that, with a dataset, type:\n",
    "\n",
    "<i>df.head()</i> \n",
    "\n",
    "That will return the first five rows. You can see more rows if you specify how many you’d like to see inside the parentheses. Try that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3fb177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f649f73f",
   "metadata": {},
   "source": [
    "You can also see a random sample of your data by typing \n",
    "\n",
    "<i>df.sample()</i>\n",
    "\n",
    "Or, the end of your data:\n",
    "\n",
    "<i>df.tail()</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1aa89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9793839",
   "metadata": {},
   "source": [
    "Any time you tell the computer to do something with a dataset, you have to tell it what dataset you want it to do that thing on. In the case above, we’re using df. If you had two datasets, you might name the other one df2 and want to run the same commands on it. \n",
    "\n",
    "Let’s try that with a list of industries that correspond with numeric codes found in the PPP loan data. (What we’re calling df.) \n",
    "\n",
    "<i>df2 = pd.read_csv('NAICS.csv')</i> \n",
    "\n",
    "Now ask the computer to display the first 10 rows and the last 10 rows of df2.\n",
    "\n",
    "You can do this with any many datasets as you are trying to analyze at once. In fact, from here, we will be creating new variables (in this case dataframes) based on the analysis we do. Think about in Excel, when you make a pivot table, it opens a new sheet. Or, if you filter a subset of your data and you open a new sheet and paste the results of your filter. We will be doing that in python, but think of each sheet as a new dataframe, or variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f00784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9064e3ca",
   "metadata": {},
   "source": [
    "# Try out a few basic commands and formatting \n",
    "You may want to start by telling the computer that you want to see all your columns when you ask to display your data. If you’ve noticed, so far, there are likely ellipses covering up a lot of the columns in the middle when you do the df.head() command. \n",
    "\n",
    "Try this\n",
    "\n",
    "#set your notebook so you see all column headers \n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "And then try df.head()\n",
    "\n",
    "This may or may not be necessary depending on how many columns you have, but it will save you a headache if you go ahead and use this code before trying to analyze anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6fd0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6546a21",
   "metadata": {},
   "source": [
    "The following are commands that you’ll likely use to get familiar with your data. \n",
    "\n",
    "Find out how many rows you have. Type the following code into the blank cell below and shift return.\n",
    "\n",
    "df.shape() \n",
    "\n",
    "The df.shape() command is important so you know how many records you have. Always keep an eye on that number in every data frame you’re working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e7618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61d316ab",
   "metadata": {},
   "source": [
    "Get the datatypes for all records. Type the following code into the blank cell below and shift return.\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb10d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd4ff212",
   "metadata": {},
   "source": [
    "This command will tell you how many non-null (meaning cells that are not empty) values are within each column and what type of data lives in every column. This is how you can find if certain columns are missing too many values to be useful and also if the columns that should be numbers are being read as numbers, and if the columns that should be text are read as strings. \n",
    "\n",
    "Numbers will be either int64 or float64. \n",
    "\n",
    "Text will show up as ‘object.’ \n",
    "\n",
    "Notice how ‘LoanNumber’ is a number?\n",
    "\n",
    "LoanNumber is the unique identifier for every single loan. Unique identifiers are often a combination of letters and numbers or just numbers. Sometimes, a numeric unique ID will start with “0”. Excel will almost always interpret such a column as a number format, and automatically cut the zero off, which is bad because then two distinct things might be counted as the same thing. (In this case, a loan with the ID of 08235 and a loan with the ID of 8235 would be counted as the same thing.) \n",
    "\n",
    "We don’t want that! Using Python with Comma-Separated-Values files (csv) can rescue you from that problem. \n",
    "\n",
    "An original csv will contain the original values in that spreadsheet. But we need to tell the computer how to read those values. So let’s try the pd.read_csv command again, but this time specify the type of data in that LoanNumber column.\n",
    "\n",
    "Try this in the cell below:\n",
    "\n",
    "df = pd.read_csv('public_150k_plus.csv', dtype={'LoanNumber': str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c887fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "001268a3",
   "metadata": {},
   "source": [
    "You could also convert data types after import. That would look like:\n",
    "\n",
    "df['LoanNumber'] = df['LoanNumber'].astype(str)\n",
    "\n",
    "Here, we are writing over the Loan Number column with text values instead of numeric. You could actually create a new column instead of overwriting an existing one by saying df[‘LoanNumberText’] = (or whatever you want to call it). Then you would have two columns for LoanNumber with one being read as text and the other read as a number. \n",
    "\n",
    "(In the future, when you want to convert something to a number, you would just replace ‘str’ with ‘numeric in the above code. If you’re specifying upon impor, you’d replace ‘str’ with ‘float’ or ‘int.)\n",
    "\n",
    "Now check the data types. Write the command that tells you datatypes in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a558ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b88f335",
   "metadata": {},
   "source": [
    "Now it’s an object! \n",
    "\n",
    "Keep in mind that another big reason you may want to specify your datatypes is when you’re merging two datasets based on one common field, and one dataframe characterises that field as text while the other characterises it as a number. In order to merge the two datasets, the columns in both datasets need to be the same type. (We’re not going to get to merging yet, so just put this thought on the backburner.)\n",
    "\n",
    "Before moving on, we should also make sure the computer knows that our DateApproved field is a date. See how it’s reading it as an object? If you wanted to sort by date, you wouldn’t be able to. \n",
    "\n",
    "Here’s how you can fix that. Write the following line of code in the blank cell below.\n",
    "\n",
    "df['DateApproved'] = pd.to_datetime(df['DateApproved'])\n",
    "\n",
    "With this, again we are overwriting the original date column. If you didn’t want to do that, you could call your variable something else and have two date fields. One that is formatted as a date and one that is not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe3712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b851190",
   "metadata": {},
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d559e",
   "metadata": {},
   "source": [
    "Now let’s have the computer do some math for us. \n",
    "\n",
    "Simply type the name of your dataframe, followed by the name of whatever column you want to work on, followed by the command.\n",
    "\n",
    "This, for example, will give us the average approval amount:\n",
    "\n",
    "df.CurrentApprovalAmount.mean()\n",
    "\n",
    "You can also try:\n",
    "\n",
    "●\t.sum()\n",
    "\n",
    "●\t.max()\n",
    "\n",
    "●\t.min()\n",
    "\n",
    "●\t.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dcb2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bc9d1c1",
   "metadata": {},
   "source": [
    "Now let’s try sorting your data so the largest value is at the top. \n",
    "\n",
    "df.sort_values('CurrentApprovalAmount', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64799d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee37fd38",
   "metadata": {},
   "source": [
    "The anatomy of this code is:\n",
    "    \n",
    "-\tdf the name of the dataframe we want to work with\n",
    "\n",
    "-\t.sort_values the command that we want to use on this data frame\n",
    "\n",
    "-\t(‘CurrentApprovalAmount’ the column we want to sort by\n",
    "     \n",
    "-\t,ascending=False tells the computer to sort largest to smallest. The default is that it will sort smallest to largest. So you have to tell it False so that it knows to do the opposite. If you replace False with True, the computer will sort with the smallest values first.\n",
    "     \n",
    "-\t) closes your parenthese so the computer knows that’s the end of that command\n",
    "\n",
    "-\t.head() will show you the data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aa0d18e",
   "metadata": {},
   "source": [
    "# Trimming unwieldy datasets\n",
    "One last thing. \n",
    "\n",
    "This dataset is unwieldy, so let’s trim it down to just the columns we care about. \n",
    "\n",
    "To do that, you will make a new variable. \n",
    "\n",
    "Let’s just call it df3. \n",
    "\n",
    "df3 = df[['BorrowerName','BorrowerCity','InitialApprovalAmount','CurrentApprovalAmount', 'RuralUrbanIndicator','Gender','JobsReported']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b876e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bc880e7",
   "metadata": {},
   "source": [
    "Every column name should be in quotes or single quotes and separated by a comma. Note the double brackets here. You need those in order to pull out the columns you want. \n",
    "\n",
    "You can also rename these. This will come in handy when column headers are too long or complicated. This time, just one set of brackets. Your names still need to be in quotes and separated by commas. You also have to make sure you name each column, even if you keep the original name. You will get an error if you’re missing one. \n",
    "\n",
    "df3.columns = ['business', 'city', 'initial_anount', 'current_amount', 'rural_urban', 'gender', 'jobs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f693a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca9555d4",
   "metadata": {},
   "source": [
    "# Saving to a csv you can open in Excel\n",
    "Now let’s save our new dataframe to a file you can open outside of this program. Run the code in the cell below and you'll find a new file in you computer that you made!\n",
    "\n",
    "df3.to_csv('loans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a83e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
